# Local Hybrid RAG System (OpenSearch + Ollama)

A fully local **Hybrid Retrieval-Augmented Generation (RAG)** system that combines
**semantic vector search + keyword search** using **OpenSearch**, and generates
responses using **Ollama-hosted LLMs**, wrapped in an interactive **Streamlit UI**.

This project demonstrates how to build a **production-style, local-first RAG pipeline**
without relying on external cloud LLM APIs.

---

## ğŸš€ Features

- ğŸ” **Hybrid Search**
  - Dense vector search (Sentence Transformers embeddings)
  - Sparse keyword search (BM25)
  - Combined relevance scoring using OpenSearch

- ğŸ¤– **Local LLM Inference**
  - Uses **Ollama** (e.g. `llama3:latest`)
  - No OpenAI / cloud dependency

- ğŸ“„ **Document Ingestion**
  - Upload PDFs
  - Chunking + embedding
  - Indexed into OpenSearch

- ğŸ§  **RAG Mode Toggle**
  - Enable / disable RAG at query time
  - Adjustable context window size

- ğŸ–¥ï¸ **Streamlit UI**
  - Chat interface
  - Upload documents
  - Temperature control
  - Search result tuning

---

## ğŸ§  Architecture Overview
This project implements a **local, hybrid Retrieval-Augmented Generation (RAG) architecture** that combines semantic vector search with keyword-based retrieval, followed by local LLM inference.    
User Query
â”‚
â–¼
Streamlit UI
â”‚
â–¼
Query Embedding
(SentenceTransformer)
â”‚
â–¼
Hybrid Retrieval (OpenSearch)
â”œâ”€â”€ Dense Vector Search (kNN)
â””â”€â”€ Sparse Keyword Search (BM25)
â”‚
â–¼
Top-K Relevant Context Chunks
â”‚
â–¼
Prompt Construction
â”‚
â–¼
Ollama (Local LLM - llama3)
â”‚
â–¼
Final Answer

### Component Breakdown

- **Streamlit UI**
  - Handles user interaction, document uploads, and configuration controls.

- **Embedding Layer**
  - Converts user queries and document chunks into dense vector representations using Sentence Transformers.

- **OpenSearch Hybrid Retrieval**
  - Performs:
    - kNN vector similarity search for semantic relevance
    - BM25 keyword search for exact term matching
  - Combines both signals to retrieve the most relevant context.

- **Context Window Builder**
  - Selects top-k retrieved chunks and formats them into a structured prompt.

- **Ollama LLM**
  - Runs a fully local LLM (`llama3`) to generate grounded responses based on retrieved context.

- **Response Generation**
  - Produces final answers with reduced hallucination by grounding responses in retrieved documents.

---

This architecture enables **cloud-free, privacy-preserving, and production-style RAG workflows** entirely on a local machine.


---

## ğŸ§© Tech Stack

| Layer | Technology |
|-----|-----------|
| UI | Streamlit |
| LLM | Ollama (`llama3`) |
| Search Engine | OpenSearch |
| Embeddings | `sentence-transformers/all-mpnet-base-v2` |
| Language | Python 3.12 |
| OS Support | Linux / WSL |

---

## ğŸ“‚ Project Structure

```text
local-hybrid-RAG-system/
â”œâ”€â”€ Welcome.py               # Streamlit entry point
â”œâ”€â”€ pages/                   # UI pages (Chatbot, Upload)
â”œâ”€â”€ src/                     # Core RAG logic
â”‚   â”œâ”€â”€ chat.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ opensearch_client.py
â”‚   â””â”€â”€ constants.py
â”œâ”€â”€ notebooks/               # Experiments & exploration
â”œâ”€â”€ images/                  # UI screenshots
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Haripranay22/local-hybrid-RAG-system.git
cd local-hybrid-RAG-system

### 2ï¸âƒ£ Create virtual environment
```bash
python3 -m venv .venv
source .venv/bin/activate

